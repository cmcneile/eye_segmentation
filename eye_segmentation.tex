%% 
%%  http://wso.williams.edu/wiki/index.php/LaTeX_Problem_Set_Template
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%This is a science homework template. Modify the preamble to suit
%%%your needs. 
%The junk text is   there for you to immediately see how the
%headers/footers look at first 
%typesetting.


\documentclass[12pt]{article}

%AMS-TeX packages
\usepackage{amssymb,amsmath,amsthm} 
%geometry (sets margin) and other useful packages
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx,ctable,booktabs}
\usepackage{url}
\usepackage{slashed}

%    
\begin{document}

\title{Segmentation of eye from MRI scans of heads}
%%\author{Craig McNeile}
\date{}

\maketitle


\section{Introduction}

%%  https://slicer.readthedocs.io/en/latest/user_guide/modules/segmentations.html
The eyes and lenses were segmentated from the MRI scans of the heads
of subjects using the 3D slicer software~\cite{kikinis20133d}.  The
segmentation editor module~\cite{pinter2019polymorph} in 3D slicer was
used to manually extract the eye and lenses.  The files were then
written to disk in STL format.  We want to automate this processs, so
that the eyes are extracted from the MRI scans using machine learning.


\section{New data sets}

New data from the team: much higher resolution hence a much easier
time segmenting the eyes, but I slight tilt of the head created a
slight imbalance in the definition of the two eyes.


The Oasis Project~\cite{van2021mri} provides MRI scans of the heads of
older patients to investigate Alzheimer's disease.  The MRI data sets
can be downloaded from this site \url{https://www.oasis-brains.org/}
The resolution of the MRI scans were not good as the data from the
dedicated measurements from the 3T MRI scanner at BRIC at the
University of Plymouth. However, it was possible to extract 3D models of the
eyes from the MRI scans we looked at.

\section{Automating the segmentation of the eyes}

We followed the example scripts written by
Mokhtari Mohammed El Amine that used segmentation to extract
livers from MRI scans. The python code was available
on github
\url{https://github.com/amine0110/Liver-Segmentation-Using-Monai-and-PyTorch}. There
is long YouTube video about the application
\url{https://www.youtube.com/watch?v=AU4KlXKKnac}.


In this video it’s shown how to train a system to compute a
segmentation using the python libraries from above. The video shows
from start to finish all the steps required for the system to work
properly. It starts with creating a DICOM series of the MRI scan of a
patient, which is a folder of images, one for each slice. Then we
create a segmentation of the part of the scan we are interested in and
we do the same: create a DICOM series. After this initial process we
set these folders of images to have a specific number of slices, in
the video was 65, and then we transform the folder of images into one
Nifti file, this is because the libraries require such type of
file. This is for the training part, we also have to the same for the
testing part, which makes sure that the training was done correctly.
In trying to compute this code I have used Google Colab~\cite{bisong2019google}
and thanks to
the GPU built in setting I managed to run the code without problem.


The example python script was built on the following libraries:

\begin{description}

  \item[monai] is a set of open-source, freely available collaborative frameworks built for accelerating research and clinical collaboration in Medical Imaging.  \url{https://monai.io/}

\item[pytorch] Pytorch~\cite{paszke2019pytorch}
  is an open source machine learning library  (\url{https://pytorch.org/}.)
    
\end{description}

The underlying deep network for segmentation was
U-net~\cite{ronneberger2015u}.  This uses many layers of Convolutional
neural networks.  U-net is a standard network architecture for many
segmentation problems and it was designed to be used with a small
number of labelled images.


\subsection{Deep Learning concepts:}

There are a variety of ways to compare the reconstructed images (see
the review~\cite{taha2015metrics} ).  The metric used in the example
tutorial was the Dice one~\cite{zijdenbos1994morphometric}.
For descrete sets $X$ and $Y$ 

\begin{equation}
\mbox{DSC} = \frac{X \cup  Y} {\mid X \mid + \mid Y \mid }
\end{equation}
  
The Weighted Cross Entropy  similarly to the Dice score,
measures how different two classes are, but instead of being two sets,
it measures how different two probability distribution are. In our
case it calculates the difference between the predicted label
distribution and the real label distribution. Let p be the predicted
label distribution and x the real label distribution and we have:

\begin{equation}
\mbox{CE} = \sum_{i=1}^{n} x_i \log(p_i)
\end{equation}
For a n-dimensinal vector $x_i$.

The weighted cross entropy (WCE) is used when we need to make sure
that the system pays attention to a specific element of class of our
data for example. To do so we add a vector of “weights” to our
calculation:

\begin{equation}
\mbox{WCE} = \sum_{i=1}^{n} W_i x_i \log(p_i)
\end{equation}

An epoch in machine learning means one complete pass of the training
dataset through the algorithm.


\subsection{Another option for segmentation}

Another possibility that requires a NVIDIA GPU is the assisted
AI-segmentation extension in 3D Slicer:
\url{https://github.com/NVIDIA/ai-assisted-annotation-client/tree/master/slicer-plugin}
.
This extension has an automated segmentation functions, but it
requires access to the NVIDIA servers, so for ethic reasons and lack
of a GPU I did not continue down that route.

\section{Conclusion}

\bibliographystyle{h-physrev5}
\bibliography{eye}


\end{document}
