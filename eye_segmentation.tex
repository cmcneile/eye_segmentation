%% 
%%  http://wso.williams.edu/wiki/index.php/LaTeX_Problem_Set_Template
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%This is a science homework template. Modify the preamble to suit
%%%your needs. 
%The junk text is   there for you to immediately see how the
%headers/footers look at first 
%typesetting.


\documentclass[12pt]{article}

%AMS-TeX packages
\usepackage{amssymb,amsmath,amsthm} 
%geometry (sets margin) and other useful packages
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx,ctable,booktabs}
\usepackage{url}
\usepackage{slashed}

%    
\begin{document}

\title{Segmentation of eye from MRI scans of heads using machine learning}
%%\author{Craig McNeile}
\date{}

\maketitle


\section{Introduction}

%%  https://slicer.readthedocs.io/en/latest/user_guide/modules/segmentations.html
The eyes and lenses were segmentated from the MRI scans of the heads
of subjects using the 3D slicer software~\cite{kikinis20133d}.  The
segmentation editor module~\cite{pinter2019polymorph} in 3D slicer was
used to manually extract the eye and lenses.  The files were then
written to disk in STL format. The aim of this project to automate the
segmentation, so that the eyes are extracted from the MRI scans
of the head using machine learning.

\section{Automating the segmentation of the eyes}

We followed the example scripts written by
Mokhtari Mohammed El Amine that used segmentation to extract
livers from MRI scans. The python code was available
on github
\url{https://github.com/amine0110/Liver-Segmentation-Using-Monai-and-PyTorch}. There
is long YouTube video about the application
\url{https://www.youtube.com/watch?v=AU4KlXKKnac}.


In the video it is shown how to train a system to compute a
segmentation using the python libraries. The video shows
from start to finish all the steps required for the system to work
properly. The problem is supervised learning. This requires that
initial MRI of the head scans with  a manually extracted set of eyes,
which form the training data set.


The trainin of the algorithm starts with creating a DICOM series of
the MRI scan of a patient, which is a folder of images, one for each
slice. Then we create a segmentation of the part of the scan we are
interested in and we do the same: create a DICOM series. After this
initial process we set these folders of images to have a specific
number of slices, in the video was 65, and then we transform the
folder of images into one Nifti file~\cite{li2016first},
this is because the libraries
require such type of file. This is for the training part, we also have
to the same for the testing part, which makes sure that the training
was done correctly.

Google Colab~\cite{bisong2019google} was used to run the algorithm
to train the algorithm to segmentate the image. Google collab
has support for running using a GPU. The code ran to completion,
but the metric to judge the accuracy did not change.


The example python script was built on the following libraries:

\begin{description}

  \item[monai] is a set of open-source, freely available collaborative frameworks built for accelerating research and clinical collaboration in Medical Imaging.  \url{https://monai.io/}

\item[pytorch] Pytorch~\cite{paszke2019pytorch}
  is an open source machine learning library  (\url{https://pytorch.org/}.)
    
\end{description}

The underlying deep network for segmentation was
U-net~\cite{ronneberger2015u}.  This uses many layers of Convolutional
neural networks, but with no dense layers.  U-net is a standard
network architecture for many segmentation problems and it was
designed to be used with a small number of labelled images.


\subsection{Deep Learning concepts:}

There are a variety of ways to compare the reconstructed images (see
the review~\cite{taha2015metrics} ).  The metric used in the example
tutorial was the Dice one~\cite{zijdenbos1994morphometric}.
For descrete sets $X$ and $Y$ 

\begin{equation}
\mbox{DSC} = \frac{X \cup  Y} {\mid X \mid + \mid Y \mid }
\end{equation}
  
The Weighted Cross Entropy  similarly to the Dice score,
measures how different two classes are, but instead of being two sets,
it measures how different two probability distribution are. In our
case it calculates the difference between the predicted label
distribution and the real label distribution. Let p be the predicted
label distribution and x the real label distribution and we have:

\begin{equation}
\mbox{CE} = \sum_{i=1}^{n} x_i \log(p_i)
\end{equation}
For a n-dimensinal vector $x_i$.

The weighted cross entropy (WCE) is used when we need to make sure
that the system pays attention to a specific element of class of our
data for example. To do so we add a vector of “weights” to our
calculation:

\begin{equation}
\mbox{WCE} = \sum_{i=1}^{n} W_i x_i \log(p_i)
\end{equation}

An epoch in machine learning means one complete pass of the training
dataset through the algorithm.


\subsection{Another option for segmentation}

Another possibility that requires a NVIDIA GPU is the assisted
AI-segmentation extension in 3D Slicer:
\url{https://github.com/NVIDIA/ai-assisted-annotation-client/tree/master/slicer-plugin}
.
This extension has an automated segmentation functions, but it
requires access to the NVIDIA servers, so for ethic reasons and lack
of a GPU I did not continue down that route.


\section{New data sets}

A new data set (taken at BRIC using the 3T MRI scanner), tagged as
24/1/2023,on the MS teams was investigated. The 3Dslicer software was
used to manually segmentate the eyes from the MRI scans. The new scans
had much higher resolution than the scans taken in 2022, so it was
easier to extract the eyes. There was a slight imbalance in the
definition of the two eyes, possibly because of a slight tilt of the
head.


The Oasis Project~\cite{van2021mri} provides MRI scans of the heads of
older patients to investigate Alzheimer's disease.  One motivation for
investigating the data from the Oasis Project was that there were data
from 400 patients, so there was potentially a large data set that
could be used for Machine Learning.  A 1.5-T Vision MRI scanner was
used (Siemens, Erlangen, Germany) in the study.  The details are
T1-weighted magnetic resonance imaging scans of the head with 128
slices.  The MRI data sets can be downloaded from this site
\url{https://www.oasis-brains.org/}


The resolution of the MRI scans, from the Oasis project, were not good
as the data from the dedicated measurements from the 3T MRI scanner at
BRIC at the University of Plymouth. However, it was possible to
extract 3D models of the eyes from the MRI scans we looked at.



\section{Conclusion}

\bibliographystyle{h-physrev5}
\bibliography{eye}


\end{document}
